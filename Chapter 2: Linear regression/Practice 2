import numpy as np
import matplotlib.pyplot as plt

data = {
    'TV': [230.1, 44.5, 17.2, 151.5, 180.8],
    'Radio': [37.8, 39.3, 45.9, 41.3, 10.8],
    'Newspaper': [69.2, 45.1, 69.3, 58.5, 58.4],
    'Sales': [22.1, 10.4, 12.0, 16.5, 17.9]
}

X = np.array([data['TV'], data['Radio'], data['Newspaper']]).T  
y = np.array(data['Sales'])

means = np.mean(X, axis=0)
stds = np.std(X, axis=0)
X_normalized = (X - means) / stds  

m, n = X_normalized.shape
weights = np.zeros(n)
bias = 0
learning_rate = 0.001
iterations = 20000
tolerance = 1e-9  

def compute_cost(X, y, weights, bias):
    m = len(y)
    predictions = X.dot(weights) + bias
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
    return cost

def gradient_descent(X, y, weights, bias, learning_rate, iterations, tolerance):
    m = len(y)
    cost_history = []

    for i in range(iterations):
        predictions = X.dot(weights) + bias
        error = predictions - y

        weights_gradient = (1 / m) * X.T.dot(error)
        bias_gradient = (1 / m) * np.sum(error)

        weights -= learning_rate * weights_gradient
        bias -= learning_rate * bias_gradient

        cost = compute_cost(X, y, weights, bias)
        cost_history.append(cost)

        # Convergence check
        if i > 0 and abs(cost_history[-2] - cost_history[-1]) < tolerance:
            print(f"Converged at iteration {i}")
            break

    return weights, bias, cost_history

weights_optimal, bias_optimal, cost_history = gradient_descent(X_normalized, y, weights, bias, learning_rate, iterations, tolerance)


#print("Optimal weights:", weights_optimal)
#print("Optimal bias:", bias_optimal)

new_data = np.array([44.5, 39.3, 45.1])
new_data_normalized = (new_data - means) / stds  
predicted_sales = new_data_normalized.dot(weights_optimal) + bias_optimal
print("Predicted sales:", predicted_sales)

#plt.plot(cost_history)
#plt.title("Cost Function over Iterations")
#plt.xlabel("Iteration")
#plt.ylabel("Cost")
#plt.grid(True)
#plt.show()
